---
title: 深度学习—正则化
date: 2017-09-23 10:38:43
tags: DeepLearning 
---
<font size=5 color=green>没有免费午餐定理：</font>
　　在所有可能的数据**生成分布上平均之后**，<font color=red>每个分类算法在未事先观测到的点上都有相同的错误率。换言之，没有一个机器学习算法总是比其他的要好。</font>
　　幸运的是，这些结论仅在我们**考虑所有可能的数据生成分布时才成立**。 在真实世界应用中，如果我们对遇到的概率分布进行假设的话，那么我们可以设计在这些分布上效果良好的学习算法。
　　这意味着机器学习研究的目标不是找一个通用的学习算法或是绝对最好的学习算法。反之，我们的目标是理解什么样的分布与人工智能获取经验的”真实世界“相关，什么样的学习算法在我们关注的数据生成分布上效果最好。
<!--more-->
<font size=5 color=green>正则化：</font>  
　　**没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法**。 我们建立一组学习算法的偏好来达到这个要求。 当这些偏好和我们希望算法解决的学习问题相吻合时，性能会更好。
　　例如，我们可以加入**权重衰减**来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和J(w)，其偏好于平方L^2范数较小的权重。 具体如下：
<center>J(w)=MSEtrain+λw⊤w</center>
 　　其中**λ是提前挑选的值，控制我们偏好小范数权重的程度。**<font color=navy>当λ=0，我们没有任何偏好。 越大的λ偏好范数越小的权重。 最小化λ可以看作是拟合训练数据和偏好小权重范数之间的权衡。</font>
在我们权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确地表示了偏好权重较小的线性函数。 有很多其他方法隐式或显式地表示对不同解的偏好。 总而言之，这些不同的方法都被称为**正则化**。
 <font color=red size=4 >** 　　机器学习中的一个核心问题是设计不仅在训练数据集上表现好，而且能在新输入上泛化好的算法。在机器学习中，许多的策略被显示的设计来减少测试误差（可能会以增大训练误差为代价）。这些策略统称为正则化。**</font>

